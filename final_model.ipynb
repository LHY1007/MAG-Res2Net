{"metadata":{"colab":{"provenance":[],"mount_file_id":"1J-AWwz0_nsDFoN9YGs5fN6hvfMQ2D_xx","authorship_tag":"ABX9TyMpWUdXrzQG9tY0cLvGTnyM"},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nfrom torch.autograd.function import Function\nimport numpy as np\nimport torch.utils.data as Data\nimport math\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport copy\nimport time\nfrom torch.optim import Adam\nimport pandas as pd\nimport numpy as np\nimport sklearn.metrics as metrics\nfrom collections import Counter\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom torch.cuda.amp import autocast as autocast\nfrom torch.cuda.amp import GradScaler\nimport matplotlib.pyplot as plt\n\nfrom torch import randperm\nfrom sklearn.utils import resample\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"XObB5T3q_SD6","executionInfo":{"status":"ok","timestamp":1676441630948,"user_tz":-480,"elapsed":1902,"user":{"displayName":"刘涵瑜","userId":"15463888377010770035"}},"execution":{"iopub.status.busy":"2023-08-25T06:51:23.123997Z","iopub.execute_input":"2023-08-25T06:51:23.124589Z","iopub.status.idle":"2023-08-25T06:51:29.392292Z","shell.execute_reply.started":"2023-08-25T06:51:23.124551Z","shell.execute_reply":"2023-08-25T06:51:29.390146Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"#We use the kaggle platform, so we need to download the Lion optimizer related packages every time.\n!pip install lion-pytorch\n!pip install triton -U --pre\nfrom lion_pytorch import Lion","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:55.995185Z","iopub.status.idle":"2023-08-25T06:50:55.998677Z","shell.execute_reply.started":"2023-08-25T06:50:55.998317Z","shell.execute_reply":"2023-08-25T06:50:55.998351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HAR_BorderlineSMOTE(Data.Dataset):\n    def __init__(self, filename_x, filename_y):\n        self.filename_x = filename_x\n        self.filename_y = filename_y\n\n    def HAR_data(self):\n        \n        data_x_raw = np.load(self.filename_x)\n        data_x=data_x_raw\n        data_y = np.load(self.filename_y)\n        data_x = torch.tensor(data_x, dtype=torch.float32)\n        data_y = torch.tensor(data_y, dtype=torch.long)\n        \n        train_data, val_data, train_label, val_label = train_test_split(data_x, data_y, test_size=0.4)\n        smo = BorderlineSMOTE(random_state=42,kind=\"borderline-1\")\n        n, nx, ny = train_data.shape\n        train_data = train_data.reshape((n,nx*ny))\n        train_data, train_label = smo.fit_resample(train_data, train_label)\n        train_data = train_data.reshape((train_data.shape[0],nx,ny))\n\n        train_data=np.transpose(train_data,(0,2,1))\n        val_data=np.transpose(val_data,(0,2,1))\n        print(Counter(train_label))\n        print(\"train_data shape:\",train_data.shape)\n        print(\"val_data shape:\",val_data.shape)\n\n        train_dataset = Data.TensorDataset(\n            torch.from_numpy(train_data), \n            torch.from_numpy(train_label))\n        val_dataset = Data.TensorDataset(val_data, val_label)\n        return train_dataset,val_dataset\n\nclass trian_HAR(Data.Dataset):\n    def __init__(self, filename_x, filename_y):\n        self.filename_x = filename_x\n        self.filename_y = filename_y\n\n    def HAR_data(self):\n        data_x_raw = np.load(self.filename_x)\n\n        data_x=data_x_raw  \n        #data_x = np.expand_dims(data_x_raw, 1)\n        #data_x = np.transpose(data_x,(0,2,1))\n        data_y = np.load(self.filename_y)\n        train_data, val_data, train_label, val_label = train_test_split(data_x, data_y, test_size=0.2)\n        \n        train_data=np.transpose(train_data,(0,2,1))\n        val_data=np.transpose(val_data,(0,2,1))\n        print(\"train_data shape:\",train_data.shape)\n        print(\"val_data shape:\",val_data.shape)\n\n        train_dataset = Data.TensorDataset(torch.from_numpy(train_data), torch.from_numpy(train_label))\n        val_dataset = Data.TensorDataset(torch.from_numpy(val_data), torch.from_numpy(val_label))\n        return train_dataset,val_dataset\n\nclass HAR(Data.Dataset):\n    def __init__(self, filename_x, filename_y):\n        self.filename_x = filename_x\n        self.filename_y = filename_y\n\n    def HAR_data(self):\n        data_x_raw = np.load(self.filename_x)\n\n        data_x=data_x_raw  # (N, C, H, W) (7352, 1, 128, 9)\n        # data_x = np.expand_dims(data_x_raw, 1)\n        data_y = np.load(self.filename_y)\n        tr1 = torch.from_numpy(data_x)\n        tr1 = tr1.permute(0, 2, 1)\n\n        torch_dataset = Data.TensorDataset(tr1, torch.from_numpy(data_y))\n        return torch_dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.006446Z","iopub.status.idle":"2023-08-25T06:50:56.012570Z","shell.execute_reply.started":"2023-08-25T06:50:56.012185Z","shell.execute_reply":"2023-08-25T06:50:56.012234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x_list = \"/kaggle/input/wisdm-data/wisdm/x_train.npy\"\ntrain_y_list = \"/kaggle/input/wisdm-data/wisdm/y_train.npy\"\ntest_x_list = \"/kaggle/input/wisdm-data/wisdm/x_test.npy\"\ntest_y_list = \"/kaggle/input/wisdm-data/wisdm/y_test.npy\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":886,"status":"ok","timestamp":1676441631832,"user":{"displayName":"刘涵瑜","userId":"15463888377010770035"},"user_tz":-480},"id":"a12F3n8bU8uI","outputId":"47e18653-6722-4484-9fb3-7b1ddb41a0b1","execution":{"iopub.status.busy":"2023-08-25T06:50:56.017051Z","iopub.status.idle":"2023-08-25T06:50:56.021831Z","shell.execute_reply.started":"2023-08-25T06:50:56.021467Z","shell.execute_reply":"2023-08-25T06:50:56.021504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#data_train= trian_HAR(train_x_list, train_y_list)\ndata_train= HAR_BorderlineSMOTE(train_x_list, train_y_list)\ntrain_dataset,val_dataset = data_train.HAR_data()\ndata_test = HAR(test_x_list, test_y_list)\ntest_dataset = data_test.HAR_data()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.023519Z","iopub.status.idle":"2023-08-25T06:50:56.024459Z","shell.execute_reply.started":"2023-08-25T06:50:56.024137Z","shell.execute_reply":"2023-08-25T06:50:56.024164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CenterLoss(nn.Module):\n    def __init__(self, num_classes, feat_dim, size_average=True):\n        super(CenterLoss, self).__init__()\n        self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))\n        self.centerlossfunc = CenterlossFunc.apply\n        self.feat_dim = feat_dim\n        self.size_average = size_average\n\n    def forward(self, label, feat):\n        batch_size = feat.size(0)\n        feat = feat.view(batch_size, -1)\n        # To check the dim of centers and features\n        if feat.size(1) != self.feat_dim:\n            raise ValueError(\"Center's dim: {0} should be equal to input feature's \\\n                            dim: {1}\".format(self.feat_dim,feat.size(1)))\n        batch_size_tensor = feat.new_empty(1).fill_(batch_size if self.size_average else 1)\n        loss = self.centerlossfunc(feat, label, self.centers, batch_size_tensor)\n        return loss\n\n\nclass CenterlossFunc(Function):\n    @staticmethod\n    def forward(ctx, feature, label, centers, batch_size):\n        ctx.save_for_backward(feature, label, centers, batch_size)\n        centers_batch = centers.index_select(0, label.long())\n        return (feature - centers_batch).pow(2).sum() / 2.0 / batch_size\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        feature, label, centers, batch_size = ctx.saved_tensors\n        centers_batch = centers.index_select(0, label.long())\n        diff = centers_batch - feature\n        # init every iteration\n        counts = centers.new_ones(centers.size(0))\n        ones = centers.new_ones(label.size(0))\n        grad_centers = centers.new_zeros(centers.size())\n\n        counts = counts.scatter_add_(0, label.long(), ones)\n        grad_centers.scatter_add_(0, label.unsqueeze(1).expand(feature.size()).long(), diff)\n        grad_centers = grad_centers/counts.view(-1, 1)\n        return - grad_output * diff / batch_size, None, grad_centers / batch_size, None","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.028162Z","iopub.status.idle":"2023-08-25T06:50:56.031127Z","shell.execute_reply.started":"2023-08-25T06:50:56.029907Z","shell.execute_reply":"2023-08-25T06:50:56.030064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if(torch.cuda.is_available()):\n    device = torch.device(\"cuda\")\n    print(\"使用GPU训练中：{}\".format(torch.cuda.get_device_name()))\nelse:\n    device = torch.device(\"cpu\")\n    print(\"使用CPU训练\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676441632903,"user":{"displayName":"刘涵瑜","userId":"15463888377010770035"},"user_tz":-480},"id":"Qj4U1DNOBGiE","outputId":"4cf38c5c-6687-4513-b65d-337c5868fd7e","execution":{"iopub.status.busy":"2023-08-25T06:50:56.034285Z","iopub.status.idle":"2023-08-25T06:50:56.037846Z","shell.execute_reply.started":"2023-08-25T06:50:56.036791Z","shell.execute_reply":"2023-08-25T06:50:56.036940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv3(in_planes, out_planes, stride=1, groups=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, groups=groups, bias=False)\n\n\ndef conv1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n\n    return nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass SEModule(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Conv1d(channels, channels // reduction, kernel_size=1, padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv1d(channels // reduction, channels, kernel_size=1, padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input):\n        x = self.avg_pool(input)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return input * x\n\nclass EfficientChannelAttention(nn.Module):           # Efficient Channel Attention module\n    def __init__(self, c, b=1, gamma=2):\n        super(EfficientChannelAttention, self).__init__()\n        t = int(abs((math.log(c, 2) + b) / gamma))\n        k = t if t % 2 else t + 1\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.conv1 = nn.Conv1d(1, 1, kernel_size=k, padding=int(k/2), bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.avg_pool(x)\n        x = self.conv1(x.transpose(-1, -2)).transpose(-1, -2)\n        out = self.sigmoid(x)\n        return out\n\nclass GatedRes2NetBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, downsample=None,\n                 stride=1, scales=4, groups=1, mod='ECA', norm_layer=None):\n        super(GatedRes2NetBottleneck, self).__init__()\n        if planes * groups % scales != 0:\n            raise ValueError('Planes must be divisible by scales')\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm1d\n        bottleneck_planes = groups * planes\n        self.conv1 = conv1(inplanes, bottleneck_planes, stride)\n        self.bn1 = norm_layer(bottleneck_planes)\n        self.conv2 = nn.ModuleList([conv3(bottleneck_planes // scales,\n                                          bottleneck_planes // scales,\n                                          groups=groups) for _ in range(scales - 1)])\n        self.bn2 = nn.ModuleList([norm_layer(bottleneck_planes // scales)\n                                  for _ in range(scales - 1)])\n        self.judge = nn.ModuleList([conv1(bottleneck_planes+2*bottleneck_planes // scales,\n                                          bottleneck_planes // scales\n                                          ) for _ in range(scales - 2)])\n        self.tanh = nn.Tanh()\n\n\n        self.conv3 = conv1(bottleneck_planes, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.model=mod\n        if mod == 'ECA':\n            self.mod = EfficientChannelAttention(self.expansion*planes)\n        elif mod == 'SE':\n            self.mod = SEModule(self.expansion*planes)\n\n        self.downsample = downsample\n        self.stride = stride\n        self.scales = scales\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        xs = torch.chunk(out, self.scales, 1)\n        ys = []\n        for s in range(self.scales):\n            if s == 0:\n                ys.append(xs[s])\n            elif s == 1:\n                ys.append(self.relu(self.bn2[s - 1](self.conv2[s - 1](xs[s]))))\n            else:\n                gate = self.tanh(self.judge[s - 2](torch.cat([out,xs[s],ys[-1]],dim=1)))\n                ys.append(self.relu(self.bn2[s - 1](self.conv2[s - 1](xs[s] + gate * ys[-1]))))\n        out = torch.cat(ys, 1)\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.model =='SE':\n            out = self.mod(out)\n        elif self.model == 'ECA':\n            x = self.mod(out)\n            out= out+x\n\n        if self.downsample is not None:\n            identity = self.downsample(identity)\n\n        out = out + identity\n        out = self.relu(out)\n\n        return out\n\nclass GatedFCN(nn.Module):\n    def __init__(self, layers, maxlength=6, groups=1,\n                  width=8,scales=4,mod='ECA', norm_layer=None,inplanes=128):\n        super(GatedFCN, self).__init__()\n        planes = [int(width * scales * 2 ** i) for i in range(4)]\n        #planes=torch.tensor(planes)\n        #planes=planes.to('cuda')\n        self.pre=inplanes\n        self.inplanes=planes[0]\n        self.maxlength=maxlength\n\n        self.pre=conv1(inplanes,planes[0])\n\n        self.layer1 = self._make_layer(GatedRes2NetBottleneck, planes[0], layers[0], scales=scales, groups=groups, mod=mod,\n                                       norm_layer=norm_layer)\n        self.layer2 = self._make_layer(GatedRes2NetBottleneck, planes[1], layers[1], stride=2, scales=scales, groups=groups,\n                                       mod=mod, norm_layer=norm_layer)\n        self.layer3 = self._make_layer(GatedRes2NetBottleneck, planes[2], layers[2], stride=2, scales=scales, groups=groups,\n                                       mod=mod, norm_layer=norm_layer)\n        self.layer4 = self._make_layer(GatedRes2NetBottleneck, planes[3], layers[3], stride=2, scales=scales, groups=groups,\n                                       mod=mod, norm_layer=norm_layer)\n        self.roi=nn.AdaptiveAvgPool1d(output_size=1)\n        self.mapping=conv1(in_planes=1024,out_planes=maxlength)\n\n        self.preluip1 = nn.PReLU()\n    def _make_layer(self, block, planes, block_num, stride=1, scales=4, groups=1, mod='ECA', norm_layer=None):\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm1d\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, downsample, stride=stride, scales=scales, groups=groups, mod=mod,\n                            norm_layer=norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, block_num):\n            layers.append(block(self.inplanes, planes, scales=scales, groups=groups, mod=mod, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self,x):\n        x1=  x.to(device)\n\n        x1=  self.pre(x1)\n        x1 = nn.functional.dropout(x1, p=0.4)\n        x1=  self.layer1(x1)\n        x1 = nn.functional.dropout(x1, p=0.4)\n        x2 = self.layer2(x1)\n        x2 = nn.functional.dropout(x2, p=0.4)\n        x3 = self.layer3(x2)\n        x2 = nn.functional.dropout(x3, p=0.4)\n        x4 = self.layer4(x3)\n        x2 = nn.functional.dropout(x4, p=0.4)\n        x5=  self.roi(x4)\n        output=self.mapping(x5)\n        output=output.squeeze()\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.043970Z","iopub.status.idle":"2023-08-25T06:50:56.044899Z","shell.execute_reply.started":"2023-08-25T06:50:56.044626Z","shell.execute_reply":"2023-08-25T06:50:56.044653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def g1(mod='ECA',inplanes=3):\n    return GatedFCN([2,3,4,1],mod=mod,inplanes=inplanes)\ndef g2(mod='ECA',inplanes=3):\n    return GatedFCN([3,4,5,1],mod=mod,inplanes=inplanes)\ndef g3(mod='ECA',inplanes=3):\n    return GatedFCN([3,4,6,3],mod=mod,inplanes=inplanes)\n\n\n#model =  g1(mod='ECA',inplanes=3).to(device)\n#model =  g2(mod='ECA',inplanes=3).to(device)\n#model =  g3(mod='ECA',inplanes=3).to(device)\n","metadata":{"id":"OaA58ghoBEVc","executionInfo":{"status":"ok","timestamp":1676441634506,"user_tz":-480,"elapsed":1605,"user":{"displayName":"刘涵瑜","userId":"15463888377010770035"}},"execution":{"iopub.status.busy":"2023-08-25T06:50:56.046285Z","iopub.status.idle":"2023-08-25T06:50:56.053653Z","shell.execute_reply.started":"2023-08-25T06:50:56.053289Z","shell.execute_reply":"2023-08-25T06:50:56.053325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 2048\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True,drop_last=True, num_workers=2,)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, pin_memory=True, shuffle=True, num_workers=2,)\nval_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, pin_memory=True, shuffle=True, num_workers=2,)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"elapsed":447,"status":"error","timestamp":1676441755689,"user":{"displayName":"刘涵瑜","userId":"15463888377010770035"},"user_tz":-480},"id":"qe3Nx04LXGFb","outputId":"4c2ed3b0-7b16-495d-fb08-8adb736d975d","execution":{"iopub.status.busy":"2023-08-25T06:50:56.055874Z","iopub.status.idle":"2023-08-25T06:50:56.057286Z","shell.execute_reply.started":"2023-08-25T06:50:56.056868Z","shell.execute_reply":"2023-08-25T06:50:56.056922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, train_dataloader,val_dataloader, num_epochs=50, lr1 = 0.001,lr2 = 0.003,loss_weight = 0.007):\n    # 定义损失函数和优化器.\n    criterion1 = nn.CrossEntropyLoss()\n    criterion2 = CenterLoss(6, 6).to(device)\n    #optimizer1 = Adam(model.parameters(), lr = lr1)\n    #optimizer2 = Adam(criterion2.parameters(), lr = lr1)\n    optimizer1 = Lion(model.parameters(), lr = lr1, weight_decay=1.0)\n    optimizer2 = Lion(criterion2.parameters(), lr = lr2, weight_decay=1.0)\n    # 定义存储训练和验证结果的列表.\n    train_loss_list = []\n    train_acc_list = []\n    val_loss_list = []\n    val_acc_list = []\n    # 定义在验证集上表现最好的模型准确率和损失.\n    best_val_acc = 0.0\n    best_val_loss = float('inf')\n    Best_epoch=0\n    # 定义存储最佳模型参数的变量.\n    best_model_params = None\n    # 开始训练模型.\n    start_time = time.time()\n    \n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        model.train()\n        for i, (inputs, labels) in enumerate(train_dataloader):\n            model.train()\n            # 将输入和标签数据转换为Tensor并放到GPU上（如果有的话）.\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # 将梯度清零.\n            optimizer1.zero_grad()\n            optimizer2.zero_grad()\n            \n            # 前向传播.\n            outputs = model(inputs)\n\n            # 计算损失和准确率.\n            loss = criterion1(outputs, labels)+loss_weight*criterion2(labels,outputs)\n            _, predicted = torch.max(outputs.data, 1)\n            train_correct += (predicted == labels).sum().item()\n            train_total += labels.size(0)\n\n            # 反向传播和更新参数.\n            loss.backward()\n            optimizer1.step()\n            optimizer2.step()\n            # 累计训练损失\n            train_loss += loss.item()\n            print(\">\",end=\"\")\n        # 计算训练准确率和损失.\n        train_acc = 100.0 * train_correct / train_total\n        train_loss = train_loss / len(train_dataloader)\n\n        # 在验证集上验证模型.\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            \n            for i, (inputs, labels) in enumerate(val_dataloader):\n                # 将输入和标签数据转换为Tensor并放到GPU上（如果有的话）.\n                model.eval()\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # 前向传播.\n                outputs = model(inputs)\n\n                # 计算损失和准确率.\n                loss = criterion1(outputs, labels)+loss_weight*criterion2(labels,outputs,)\n                _, predicted = torch.max(outputs.data, 1)\n                val_correct += (predicted == labels).sum().item()\n                val_total += labels.size(0)\n\n                # 累计验证损失.\n                val_loss += loss.item()\n\n        \n        # 计算验证准确率和损失.\n        val_acc = 100.0 * val_correct / val_total\n        val_loss = val_loss / len(val_dataloader)\n        if val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss):\n            Best_epoch =epoch+1\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            best_model_params = model.state_dict()\n        \n        print()\n        # 打印训练和验证结果.\n        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Val Loss: {:.4f}, Val Acc: {:.2f}%'\n              .format(epoch+1, num_epochs, train_loss, train_acc, val_loss, val_acc))\n        \n        # 保存训练和验证结果.\n        train_loss_list.append(train_loss)\n        train_acc_list.append(train_acc)\n        val_loss_list.append(val_loss)\n        val_acc_list.append(val_acc)\n    print(\"The best epoch:\",Best_epoch,\"    Acc:\",best_val_acc)\n    model.load_state_dict(best_model_params)\n    # 返回训练和验证结果.\n    return model.eval(),train_loss_list, train_acc_list, val_loss_list, val_acc_list","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.060159Z","iopub.status.idle":"2023-08-25T06:50:56.063765Z","shell.execute_reply.started":"2023-08-25T06:50:56.063488Z","shell.execute_reply":"2023-08-25T06:50:56.063517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model =  g1(mod='ECA',inplanes=3).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.065154Z","iopub.status.idle":"2023-08-25T06:50:56.066019Z","shell.execute_reply.started":"2023-08-25T06:50:56.065767Z","shell.execute_reply":"2023-08-25T06:50:56.065793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\nmodel,train_loss_list, train_acc_list, val_loss_list, val_acc_list=\\\ntrain(model, train_dataloader, val_dataloader, num_epochs=30,lr1 = 0.01,lr2 = 0.01,loss_weight = 0)\nend_time = time.time()\nuse_time= end_time - start_time\nprint(\"Train and val complete in {:.0f}m {:.0f}s\".format(use_time // 60, use_time % 60)) \n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCzfxu3EAqwy","outputId":"3489ed96-850a-4711-e4bf-d57f5741c9dc","executionInfo":{"status":"ok","timestamp":1676441193462,"user_tz":-480,"elapsed":6745760,"user":{"displayName":"刘涵瑜","userId":"15463888377010770035"}},"execution":{"iopub.status.busy":"2023-08-25T06:50:56.070159Z","iopub.status.idle":"2023-08-25T06:50:56.071089Z","shell.execute_reply.started":"2023-08-25T06:50:56.070828Z","shell.execute_reply":"2023-08-25T06:50:56.070853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save({'model': model.state_dict()}, 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.072438Z","iopub.status.idle":"2023-08-25T06:50:56.074319Z","shell.execute_reply.started":"2023-08-25T06:50:56.074031Z","shell.execute_reply":"2023-08-25T06:50:56.074058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndef plot_loss_and_acc(train_loss_list, train_acc_list, val_loss_list, val_acc_list):\n    # 绘制训练和验证损失\n    plt.figure(figsize=(10, 5),dpi=480)\n    plt.plot(train_loss_list, label='train_loss')\n    plt.plot(val_loss_list, label='val_loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    plt.savefig('loss')\n\n    # 绘制训练和验证准确率\n    plt.figure(figsize=(10, 5),dpi=480)\n    plt.plot(train_acc_list, label='train_acc')\n    plt.plot(val_acc_list, label='val_acc')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n    plt.savefig('acc')\nplot_loss_and_acc(train_loss_list, train_acc_list, val_loss_list, val_acc_list)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.081669Z","iopub.status.idle":"2023-08-25T06:50:56.083603Z","shell.execute_reply.started":"2023-08-25T06:50:56.083068Z","shell.execute_reply":"2023-08-25T06:50:56.083143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\ndef test_final(model, test_dataloader,loss_weight = 0.007):\n    # 将模型设置为测试模式.\n    model.eval()\n\n       # 定义损失函数和优化器.\n    criterion1 = nn.CrossEntropyLoss()\n    criterion2 = CenterLoss(6, 6).to(device)\n\n    loss_weight=0.007\n    # 在测试集上测试模型.\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    y_true = []\n    y_pred = []\n    num_classes = 6\n    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)  # 创建混淆矩阵\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(test_dataloader):\n            # 将输入和标签数据转换为Tensor并放到GPU上（如果有的话）.\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # 前向传播.\n            outputs = model(inputs)\n            pre_lab = torch.argmax(outputs, 1)\n            # 计算损失和准确率.\n            #loss = criterion1(outputs, labels)\n            loss = criterion1(outputs, labels) + loss_weight * criterion2(labels, outputs)\n            _, predicted = torch.max(\n                                     outputs.data, 1)\n            test_correct += (predicted == labels).sum().item()\n            test_total += labels.size(0)\n\n            # 累计测试损失.\n            test_loss += loss.item()\n            y_true.extend(labels.tolist())\n            y_pred.extend(pre_lab.tolist())\n            conf_matrix += confusion_matrix(labels.cpu(), pre_lab.cpu(), labels=range(num_classes))\n    report = classification_report(y_true, y_pred,digits=4)\n    g_mean = np.sqrt(np.diag(conf_matrix) / np.sum(conf_matrix, axis=1))\n    g_mean = np.mean(g_mean)\n\n    # 更新分类报告\n    report += '\\nG-mean: {:.4f}'.format(g_mean)\n    # 计算测试准确率和损失.\n    test_acc = 100.0 * test_correct / test_total\n    test_loss = test_loss / len(test_dataloader)\n\n    # 打印测试结果.\n    print('Test Loss: {:.4f}, Test Acc: {:.2f}%'.format(test_loss, test_acc))\n    print(report)\n    # 返回测试结果.\n    return test_loss, test_acc","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.085306Z","iopub.status.idle":"2023-08-25T06:50:56.086360Z","shell.execute_reply.started":"2023-08-25T06:50:56.086020Z","shell.execute_reply":"2023-08-25T06:50:56.086053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc=test_final(model, test_dataloader,loss_weight = 0.007)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.087928Z","iopub.status.idle":"2023-08-25T06:50:56.088779Z","shell.execute_reply.started":"2023-08-25T06:50:56.088513Z","shell.execute_reply":"2023-08-25T06:50:56.088541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nclass DrawConfusionMatrix:\n    def __init__(self, labels_name, normalize=True):\n        self.normalize = normalize\n        self.labels_name = labels_name\n        self.num_classes = len(labels_name)\n        self.matrix = np.zeros((self.num_classes, self.num_classes), dtype=\"float32\")\n\n    def update(self, labels, predicts):\n\n        for predict, label in zip(labels, predicts):\n            self.matrix[label, predict] += 1\n\n    def getMatrix(self,normalize=True):\n        if normalize:\n            per_sum = self.matrix.sum(axis=1)  # 计算每行的和，用于百分比计算\n            for i in range(self.num_classes):\n                self.matrix[i] =(self.matrix[i] / per_sum[i])   # 百分比转换\n            self.matrix=np.around(self.matrix, 4)   # 保留2位小数点\n            self.matrix[np.isnan(self.matrix)] = 0.0  # 可能存在NaN，将其设为0\n        return self.matrix\n\n    def drawMatrix(self):\n        self.matrix = self.getMatrix(self.normalize)\n        plt.figure(dpi=480)\n        plt.imshow(self.matrix, cmap=plt.cm.Blues)  # 仅画出颜色格子，没有值\n        plt.title(\"WISDM\")  # title\n        plt.xlabel(\"Predict label\")\n        plt.ylabel(\"Truth label\")\n\n        plt.yticks(range(self.num_classes), self.labels_name)  # y轴标签\n        plt.xticks(range(self.num_classes), self.labels_name, rotation=30)  # x轴标签\n        thresh = self.matrix.max() / 2.\n        for x in range(self.num_classes):\n            for y in range(self.num_classes):\n                #value = float(format('%.4f' % self.matrix[y, x]))*100.00 # 数值处理\n                value = str(format('%.2f' %float(self.matrix[y, x]*100.00)))+'%' # 数值处理\n                plt.text(x, y, value, verticalalignment='center', horizontalalignment='center',color=\"white\" if self.matrix[y, x] > thresh else \"black\")  # 写值\n\n        plt.tight_layout()  # 自动调整子图参数，使之填充整个图像区域\n\n        plt.colorbar()  # 色条\n        plt.savefig('./ConfusionMatrix.png', bbox_inches='tight')  # bbox_inches='tight'可确保标签信息显示全\n        plt.show()\n\n        \ndef printMatrix(test_loader, model):\n    labels_name=[\"Sitting\", \"Upstairs\", \"Downstairs\",\"Walking\", \"Standing\", \"Laying\"]\n    #labels_name=[\"Walking\", \"Walk upstairs\", \"Walk downstairs\", \"Sitting\", \"Standing\", \"Laying down\"]\n\n    drawconfusionmatrix = DrawConfusionMatrix(labels_name=labels_name)  # 实例化\n    for index, (imgs,labels ) in enumerate(test_loader,1):\n        labels_pd= model(imgs.float())\n        predict_np = np.argmax(labels_pd.cpu().detach().numpy(), axis=-1)   # array([0,5,1,6,3,...],dtype=int64)\n        labels_np = labels.numpy()                                    # array([0,5,0,6,2,...],dtype=int64)\n        drawconfusionmatrix.update(labels_np, predict_np)   # 将新批次的predict和label更新（保存）\n\n    drawconfusionmatrix.drawMatrix()  # 根据所有predict和label，画出混淆矩阵\n\n    confusion_mat=drawconfusionmatrix.getMatrix() # 你也可以使用该函数获取混淆矩阵(ndarray)\n    print(confusion_mat)\nprint(\"开始绘制混淆矩阵\")\nprintMatrix(test_dataloader, model)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:50:56.092796Z","iopub.status.idle":"2023-08-25T06:50:56.093771Z","shell.execute_reply.started":"2023-08-25T06:50:56.093476Z","shell.execute_reply":"2023-08-25T06:50:56.093506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}